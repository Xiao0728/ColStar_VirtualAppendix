{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dated-logan",
   "metadata": {},
   "source": [
    "# This notebook demonstrate the experiments addressing our RQ1 in the paper, in particular, including the following research questions:\n",
    "\n",
    "- RQ1.1: Can we reproduce the training of ColBERT?\n",
    "\n",
    "- RQ1.2: What is the impact of the similarity function for ColBERT?\n",
    "\n",
    "# Load the index and the trained checkpoint\n",
    "\n",
    "Note: for the cosine trained model, you need load the corresponding cosine index. Similarly, conduct the same practice for l2 model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "accompanied-sitting",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTerrier 0.9.1 has loaded Terrier 5.7 (built by craigm on 2022-11-10 18:30) and terrier-helper 0.0.7\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pyterrier as pt\n",
    "pt.init(tqdm='notebook')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "correct-flood",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyterrier_colbert.ranking import ColBERTFactory\n",
    "checkpoint_loc = \"/path/to/checkpoint.dnn\"\n",
    "index_path = \"/path/to/index/folder/\"\n",
    "index_name = \"index_name\"\n",
    "\n",
    "factory = ColBERTFactory(\n",
    "    checkpoint_loc,\n",
    "    index_path,\n",
    "    index_name,faiss_partitions=100,memtype='mem'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "corrected-farmer",
   "metadata": {},
   "outputs": [],
   "source": [
    "factory.faiss_index_on_gpu = False\n",
    "e2e_cosine = factory.end_to_end()\n",
    "fnt=factory.nn_term(df=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "floppy-metadata",
   "metadata": {},
   "source": [
    "# Evaluation on Dev.Small\n",
    "- following the original ColBERT paper, we reproduce the results on Dev.Small using the following pipelines:\n",
    "- rerank: performing reranking on the official BM25 reranking result set obtained from the MSMARCO leaderboard https://microsoft.github.io/msmarco/  \n",
    "- e2e: performing the end-to-end runs for each colbert-cosine and colbert-l2 model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "actual-orbit",
   "metadata": {},
   "outputs": [],
   "source": [
    "offical_bm25 = pd.read_csv(\"https://msmarco.blob.core.windows.net/msmarcoranking/top1000.dev.tar.gz\",sep=\"\\t\",names=['qid','docno','query','text'])\n",
    "offical_bm25.qid = offical_bm25.qid.astype(str)\n",
    "offical_bm25.docno = offical_bm25.docno.astype(str)\n",
    "rerank = pt.transformer.SourceTransformer(offical_bm25)>>factory.text_scorer()\n",
    "e2e = factory.end_to_end()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "recorded-address",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyterrier.measures import *\n",
    "pt.Experiment(\n",
    "    [rerank, e2e,\n",
    "    ],\n",
    "    pt.get_dataset(\"trec-deep-learning-passages\").get_topics('dev.small')\n",
    "    pt.get_dataset(\"trec-deep-learning-passages\").get_qrels('dev.small'),\n",
    "    batch_size=100, \n",
    "    verbose=True,\n",
    "    save_dir = \"./\",\n",
    "    filter_by_qrels=True,\n",
    "    eval_metrics=[RR@10, nDCG@10, R@50, R@200, R@1000],\n",
    "    names=[\"colbert.cosine.rerank.dev.small\",\"colbert.cosine.e2e.dev.small\" ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "built-reward",
   "metadata": {},
   "source": [
    "### Running experiments on Dev.Small will take some time, or you can directly using our results to perform the validation on Dev (Reproduce our results in Table1 in our paper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "acceptable-jordan",
   "metadata": {},
   "outputs": [],
   "source": [
    "e2e_cosine = pt.io.read_results(\"./colbert.cosine.e2e.dev.small.res.gz\")\n",
    "\n",
    "e2e_l2 = pt.io.read_results(\"./colbert.l2.e2e.dev.small.res.gz\")\n",
    "\n",
    "rerank_cosine = pt.io.read_results(\"./colbert.cosine.rerank.dev.small.res.gz\")\n",
    "\n",
    "rerank_l2 = pt.io.read_results(\"./colbert.l2.rerank.dev.small.res.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "sealed-employee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f72fa059a244570bab3611855fab604",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='pt.Experiment'), FloatProgress(value=0.0, max=280.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>RR@10</th>\n",
       "      <th>R@50</th>\n",
       "      <th>R@200</th>\n",
       "      <th>R@1000</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>rerank_cosine</td>\n",
       "      <td>0.3479</td>\n",
       "      <td>0.7527</td>\n",
       "      <td>0.8036</td>\n",
       "      <td>0.8140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>rerank_l2</td>\n",
       "      <td>0.3492</td>\n",
       "      <td>0.7541</td>\n",
       "      <td>0.8053</td>\n",
       "      <td>0.8140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>e2e_cosine</td>\n",
       "      <td>0.3575</td>\n",
       "      <td>0.8229</td>\n",
       "      <td>0.9109</td>\n",
       "      <td>0.9516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>e2e_l2</td>\n",
       "      <td>0.3606</td>\n",
       "      <td>0.8324</td>\n",
       "      <td>0.9232</td>\n",
       "      <td>0.9648</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            name   RR@10    R@50   R@200  R@1000\n",
       "0  rerank_cosine  0.3479  0.7527  0.8036  0.8140\n",
       "1      rerank_l2  0.3492  0.7541  0.8053  0.8140\n",
       "2     e2e_cosine  0.3575  0.8229  0.9109  0.9516\n",
       "3         e2e_l2  0.3606  0.8324  0.9232  0.9648"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from pyterrier.measures import *\n",
    "res = pt.Experiment(\n",
    "    [\n",
    "        rerank_cosine, rerank_l2,\n",
    "        e2e_cosine, e2e_l2,\n",
    "    ],\n",
    "    pt.get_dataset(\"trec-deep-learning-passages\").get_topics('dev.small'),\n",
    "    pt.get_dataset(\"trec-deep-learning-passages\").get_qrels('dev.small'),\n",
    "    batch_size=100, \n",
    "    verbose=True,round=4,\n",
    "    filter_by_qrels=True,\n",
    "    eval_metrics=[RR@10, R@50, R@200, R@1000],\n",
    "    names=[\"rerank_cosine\",\"rerank_l2\",\n",
    "           \"e2e_cosine\",\"e2e_l2\" ]\n",
    ")\n",
    "\n",
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "timely-melbourne",
   "metadata": {},
   "source": [
    "# Besides MSMARCO Dev.Small, we can also perform evaluation on TREC queries. \n",
    "\n",
    "In the following, we demonstrate how to contruct the retrieval pipelines and perform experiments on both TREC 2019 as well as TREC 2020 queries.\n",
    "\n",
    "- Note: the reranking pipeline is different from Dev.Small experiment, here we perform ColBERT reranking on top of BM25 stemmed retrieval.\n",
    "\n",
    "### Evaluation on TREC DL2019 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "incorporate-brake",
   "metadata": {},
   "outputs": [],
   "source": [
    "bm25_terrier_stemmed_text = pt.BatchRetrieve.from_dataset(\n",
    "    'msmarco_passage', \n",
    "    'terrier_stemmed_text', \n",
    "    wmodel='BM25',\n",
    "    metadata=['docno', 'text'], \n",
    "    num_results=1000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "subtle-championship",
   "metadata": {},
   "outputs": [],
   "source": [
    "rerank = (bm25_terrier_stemmed_text >>factory.text_scorer())\n",
    "e2e = factory.end_to_end()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "widespread-stephen",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyterrier.measures import *\n",
    "pt.Experiment(\n",
    "    [rerank, e2eT,\n",
    "    ],\n",
    "    pt.get_dataset(\"trec-deep-learning-passages\").get_topics('test-2019')\n",
    "    pt.get_dataset(\"trec-deep-learning-passages\").get_qrels('test-2019'),\n",
    "    batch_size=100, \n",
    "    verbose=True,\n",
    "    save_dir = \"./\",\n",
    "    filter_by_qrels=True,\n",
    "    eval_metrics=[RR@10, nDCG@10, R@50, R@200, R@1000],\n",
    "    names=[\"colbert.cosine.rerank.dl19\",\"colbert.cosine.e2e.dl19\" ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "automated-thriller",
   "metadata": {},
   "source": [
    "### Evaluation on TREC DL2020 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "excessive-seven",
   "metadata": {},
   "outputs": [],
   "source": [
    "rerank = (bm25_terrier_stemmed_text >>factory.text_scorer())\n",
    "e2e = factory.end_to_end()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "genetic-territory",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyterrier.measures import *\n",
    "pt.Experiment(\n",
    "    [rerank, e2eT,\n",
    "    ],\n",
    "    pt.get_dataset(\"trec-deep-learning-passages\").get_topics('test-2020')\n",
    "    pt.get_dataset(\"trec-deep-learning-passages\").get_qrels('test-2020'),\n",
    "    batch_size=100, \n",
    "    verbose=True,\n",
    "    save_dir = \"./\",\n",
    "    filter_by_qrels=True,\n",
    "    eval_metrics=[RR@10, nDCG@10, R@50, R@200, R@1000],\n",
    "    names=[\"colbert.cosine.rerank.dl20\",\"colbert.cosine.e2e.dl20\" ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aggregate-rugby",
   "metadata": {},
   "source": [
    "### Similarly, instead of conducting the above experiments, you can validate our reported results in Table 1 by directly using the result files we have provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "systematic-quest",
   "metadata": {},
   "outputs": [],
   "source": [
    "e2e_cosine = pt.io.read_results(\"./TREC.Res/colbert.cosine.e2e.dl19.res.gz\")\n",
    "\n",
    "e2e_l2 = pt.io.read_results(\"./TREC.Res/colbert.l2.e2e.dl19.res.gz\")\n",
    "\n",
    "rerank_cosine = pt.io.read_results(\"./TREC.Res/colbert.cosine.rerank.dl19.res.gz\")\n",
    "\n",
    "rerank_l2 = pt.io.read_results(\"./TREC.Res/colbert.l2.rerank.dl19.res.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "expressed-princess",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "162e67626f4547218554e93af2fb831c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='pt.Experiment'), FloatProgress(value=0.0, max=4.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>RR(rel=2)@10</th>\n",
       "      <th>nDCG@10</th>\n",
       "      <th>AP(rel=2)@1000</th>\n",
       "      <th>R(rel=2)@1000</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>rerank_cosine_dl19</td>\n",
       "      <td>0.8469</td>\n",
       "      <td>0.7132</td>\n",
       "      <td>0.4587</td>\n",
       "      <td>0.7553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>rerank_l2_dl19</td>\n",
       "      <td>0.8624</td>\n",
       "      <td>0.7129</td>\n",
       "      <td>0.4702</td>\n",
       "      <td>0.7553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>e2e_cosine_dl19</td>\n",
       "      <td>0.8574</td>\n",
       "      <td>0.7077</td>\n",
       "      <td>0.4445</td>\n",
       "      <td>0.7730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>e2e_l2_dl19</td>\n",
       "      <td>0.8702</td>\n",
       "      <td>0.7216</td>\n",
       "      <td>0.4620</td>\n",
       "      <td>0.8230</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 name  RR(rel=2)@10  nDCG@10  AP(rel=2)@1000  R(rel=2)@1000\n",
       "0  rerank_cosine_dl19        0.8469   0.7132          0.4587         0.7553\n",
       "1      rerank_l2_dl19        0.8624   0.7129          0.4702         0.7553\n",
       "2     e2e_cosine_dl19        0.8574   0.7077          0.4445         0.7730\n",
       "3         e2e_l2_dl19        0.8702   0.7216          0.4620         0.8230"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from pyterrier.measures import *\n",
    "DL19_res = pt.Experiment(\n",
    "    [\n",
    "        rerank_cosine, rerank_l2,\n",
    "        e2e_cosine, e2e_l2,\n",
    "    ],\n",
    "    pt.get_dataset(\"trec-deep-learning-passages\").get_topics('test-2019'),\n",
    "    pt.get_dataset(\"trec-deep-learning-passages\").get_qrels('test-2019'),\n",
    "    batch_size=100, \n",
    "    verbose=True,round=4,\n",
    "    filter_by_qrels=True,\n",
    "    eval_metrics=[RR(rel=2)@10, nDCG@10, AP(rel=2)@1000, R(rel=2)@1000],\n",
    "    names=[\"rerank_cosine_dl19\",\"rerank_l2_dl19\",\n",
    "           \"e2e_cosine_dl19\",\"e2e_l2_dl19\" ]\n",
    ")\n",
    "\n",
    "DL19_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "identical-silence",
   "metadata": {},
   "outputs": [],
   "source": [
    "e2e_cosine = pt.io.read_results(\"./TREC.Res/colbert.cosine.e2e.dl20.res.gz\")\n",
    "\n",
    "e2e_l2 = pt.io.read_results(\"./TREC.Res/colbert.l2.e2e.dl20.res.gz\")\n",
    "\n",
    "rerank_cosine = pt.io.read_results(\"./TREC.Res/colbert.cosine.rerank.dl20.res.gz\")\n",
    "\n",
    "rerank_l2 = pt.io.read_results(\"./TREC.Res/colbert.l2.rerank.dl20.res.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "stuck-extent",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79426adb5e0745b4bd22ec83455dd42d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='pt.Experiment'), FloatProgress(value=0.0, max=4.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>RR(rel=2)@10</th>\n",
       "      <th>nDCG@10</th>\n",
       "      <th>AP(rel=2)@1000</th>\n",
       "      <th>R(rel=2)@1000</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>rerank_cosine_dl20</td>\n",
       "      <td>0.8349</td>\n",
       "      <td>0.7068</td>\n",
       "      <td>0.4838</td>\n",
       "      <td>0.8072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>rerank_l2_dl20</td>\n",
       "      <td>0.8284</td>\n",
       "      <td>0.6979</td>\n",
       "      <td>0.4827</td>\n",
       "      <td>0.8072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>e2e_cosine_dl20</td>\n",
       "      <td>0.8318</td>\n",
       "      <td>0.6899</td>\n",
       "      <td>0.4725</td>\n",
       "      <td>0.8057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>e2e_l2_dl20</td>\n",
       "      <td>0.8228</td>\n",
       "      <td>0.6853</td>\n",
       "      <td>0.4747</td>\n",
       "      <td>0.8386</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 name  RR(rel=2)@10  nDCG@10  AP(rel=2)@1000  R(rel=2)@1000\n",
       "0  rerank_cosine_dl20        0.8349   0.7068          0.4838         0.8072\n",
       "1      rerank_l2_dl20        0.8284   0.6979          0.4827         0.8072\n",
       "2     e2e_cosine_dl20        0.8318   0.6899          0.4725         0.8057\n",
       "3         e2e_l2_dl20        0.8228   0.6853          0.4747         0.8386"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from pyterrier.measures import *\n",
    "DL20_res = pt.Experiment(\n",
    "    [\n",
    "        rerank_cosine, rerank_l2,\n",
    "        e2e_cosine, e2e_l2,\n",
    "    ],\n",
    "    pt.get_dataset(\"trec-deep-learning-passages\").get_topics('test-2020'),\n",
    "    pt.get_dataset(\"trec-deep-learning-passages\").get_qrels('test-2020'),\n",
    "    batch_size=100, \n",
    "    verbose=True,round=4,\n",
    "    filter_by_qrels=True,\n",
    "    eval_metrics=[RR(rel=2)@10, nDCG@10, AP(rel=2)@1000, R(rel=2)@1000],\n",
    "    names=[\"rerank_cosine_dl20\",\"rerank_l2_dl20\",\n",
    "           \"e2e_cosine_dl20\",\"e2e_l2_dl20\" ]\n",
    ")\n",
    "\n",
    "DL20_res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "imperial-status",
   "metadata": {},
   "source": [
    "### Conclusion:\n",
    "\n",
    "Overall, from the results on both Dev.Small and TREC quey sets, we find that we are able to successfully reproduce the performance of ColBERT on various query sets. In addition, several ablation studies show that more training interactions still helps improve the retrieval effectiveness of ColBERT. The L2 similarity function gives higher performance than cosine for the end-to-end setting and exhibits comparable performance for the reranking retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "advanced-proceeding",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
